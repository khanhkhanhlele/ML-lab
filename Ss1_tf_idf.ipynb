{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ss1 tf idf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPscb0WczbSlPcXwC9PAsm3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanhkhanhlele/ML-lab/blob/main/Ss1_tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import math \n",
        "import re"
      ],
      "metadata": {
        "id": "Cc_tG1z1H_VG"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfkvw4PVHkRw",
        "outputId": "480c64e0-d856-42a1-84b5-39e4e0cdf9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlrNPZpcJNeT",
        "outputId": "a94c93eb-3085-4078-8bc5-fd093cf97da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_data():\n",
        "    def gather_20newgroup_data():\n",
        "        path = 'datasets/20news-bydate'\n",
        "        dirs = [path + '/' + dir_name for dir_name in os.listdir(path) if not os.path.isfile(path+dir_name)]\n",
        "        train_dir, test_dir = (dirs[0], dirs[1]) if 'train' in dirs[0] else (dirs[1], dirs[0])\n",
        "        list_newsgroups = [newsgroup for newsgroup in os.listdir(train_dir)]\n",
        "        list_newsgroups.sort()\n",
        "        return train_dir, test_dir, list_newsgroups\n",
        "\n",
        "    def collect_data_from(parent_dir, newsgroup_list):\n",
        "        data = []\n",
        "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
        "            label = group_id\n",
        "            dir_path = parent_dir + '/' + newsgroup + '/'\n",
        "            files = [(filename, dir_path+filename) for filename in os.listdir(dir_path) if os.path.isfile(dir_path + filename)]\n",
        "            files.sort()\n",
        "            for filename, filepath in files:\n",
        "                with open(filepath, encoding='utf8', errors='ignore') as f:\n",
        "                    text = f.read().lower()\n",
        "                    words = [stemmer.stem(word) for word in re.split('\\W+', text) if word not in stop_words]\n",
        "                    content = ' '.join(words)\n",
        "                    assert len(content.splitlines())==1\n",
        "                    data.append(str(label) + '<fff>'+ filename + '<fff>' + content)\n",
        "        return data\n",
        "\n",
        "    with open('datasets/20news-bydate/stop_words.txt', encoding='unicode_escape') as f:\n",
        "        stop_words = f.read().splitlines()\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    train_dir, test_dir, list_newsgroups = gather_20newgroup_data()\n",
        "    train_data = collect_data_from(parent_dir=train_dir, newsgroup_list=list_newsgroups)\n",
        "    test_data  = collect_data_from(parent_dir=test_dir, newsgroup_list=list_newsgroups)\n",
        "    full_data = train_data + test_data\n",
        "    with open('datasets/20news-bydate/20news-train-processed.txt', 'w') as f:\n",
        "        f.write('\\n'.join(train_data))\n",
        "    with open('datasets/20news-bydate/20news-test-processed.txt', 'w') as f:\n",
        "        f.write('\\n'.join(test_data))\n",
        "    with open('datasets/20news-bydate/20news-full-processed.txt', 'w') as f:\n",
        "        f.write('\\n'.join(full_data))"
      ],
      "metadata": {
        "id": "iZoGQo7BIQO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collect_data()"
      ],
      "metadata": {
        "id": "eRaUQEJFJPuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_vocabulary(data_path):\n",
        "    def compute_idf(df, corpus_size):\n",
        "        assert df>0\n",
        "        return np.log10(corpus_size*1./df)\n",
        "    \n",
        "    with open(data_path, encoding='unicode_escape') as f:\n",
        "        lines = f.read().splitlines()\n",
        "    doc_count = defaultdict(int)\n",
        "    corpus_size = len(lines)\n",
        "\n",
        "    for line in lines:\n",
        "        features = line.split('<fff>')\n",
        "        text = features[-1]\n",
        "        words = list(set(text.split()))\n",
        "        for word in words:\n",
        "            doc_count[word] += 1\n",
        "\n",
        "    words_idfs = [(word, compute_idf(document_freq, corpus_size))\\\n",
        "                  for word, document_freq in zip(doc_count.keys(), doc_count.values())\\\n",
        "                      if document_freq > 10 and not word.isdigit()]\n",
        "    words_idfs.sort(key=(lambda x:-x[1]))\n",
        "    print(f'Vocabulary size: {len(words_idfs)}')\n",
        "    with open('datasets/20news-bydate/words_idfs.txt', 'w') as f:\n",
        "        f.write('\\n'.join([word + '<fff>' + str(idf) for word, idf in words_idfs]))"
      ],
      "metadata": {
        "id": "b7ZqLY3fR0Pp"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_vocabulary('datasets/20news-bydate/20news-full-processed.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jodaDFurTHxo",
        "outputId": "47fba341-552a-40d9-9a03-f3267b70a9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 14239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tf_idf(data_path):\n",
        "    with open('datasets/20news-bydate/words_idfs.txt', encoding='unicode_escape') as f:\n",
        "        words_idfs = [(line.split('<fff>')[0], float(line.split('<fff>')[1]))\\\n",
        "                      for line in f.read().splitlines()]\n",
        "        word_IDs = dict([(word, index) \\\n",
        "                         for index, (word, idf) in enumerate(words_idfs)])\n",
        "        idfs = dict(words_idfs)\n",
        "        with open(data_path, encoding='unicode_escape') as f:\n",
        "            documents = [(int(line.split('<fff>')[0]),\\\n",
        "                          int(line.split('<fff>')[1]),\\\n",
        "                          line.split('<fff>')[2])\\\n",
        "                          for line in f.read().splitlines()]\n",
        "        data_tf_idf = []\n",
        "        for document in documents:\n",
        "            label, doc_id, text = document\n",
        "            words = [word for word in text.split() if word in idfs]\n",
        "            word_set = list(set(words))\n",
        "            max_term_freq = max([words.count(word)\\\n",
        "                                 for word in word_set])\n",
        "            words_tfidfs = []\n",
        "            sum_squares = 0.0\n",
        "            for word in word_set:\n",
        "                term_freq = words.count(word)\n",
        "                tf_idf_value = term_freq * 1./max_term_freq * idfs[word]\n",
        "                words_tfidfs.append((word_IDs[word], tf_idf_value))\n",
        "                sum_squares += tf_idf_value**2\n",
        "            words_tfidfs_normalized = [str(index)+':'+str(tf_idf_value/np.sqrt(sum_squares))\\\n",
        "                                       for index, tf_idf_value in words_tfidfs]\n",
        "            sparse_rep = ' '.join(words_tfidfs_normalized)\n",
        "            data_tf_idf.append((label, doc_id, sparse_rep))\n",
        "    return data_tf_idf"
      ],
      "metadata": {
        "id": "vNAHj69dXgy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_tf_idf = get_tf_idf('datasets/20news-bydate/20news-full-processed.txt')"
      ],
      "metadata": {
        "id": "nZOoINclbGBQ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"datasets/20news-bydate/data_tf_idf.txt\", 'w+') as f:\n",
        "    result = []\n",
        "    for num in range(len(data_tf_idf)):\n",
        "        connect = \"<fff>\".join([str(data_tf_idf[num][0]), str(data_tf_idf[num][1]), data_tf_idf[num][2]])\n",
        "        result.append(connect)\n",
        "    f.write('\\n'.join(result))"
      ],
      "metadata": {
        "id": "fdsJ-0rbfC_f"
      },
      "execution_count": 64,
      "outputs": []
    }
  ]
}